{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8af48ca1-b6d1-4092-b7b5-037d3c2d7aef",
   "metadata": {
    "id": "8af48ca1-b6d1-4092-b7b5-037d3c2d7aef"
   },
   "source": [
    "# M4 | Research Investigation Notebook\n",
    "\n",
    "In this notebook, you will do a research investigation of your chosen dataset in teams. You will begin by formally selecting your research question (task 0), then processing your data (task 1), creating a predictive model (task 2), evaluating your model's results (task 3), and describing the contributions of each team member (task 4).\n",
    "\n",
    "For grading, please make sure your notebook has all cells already run. You will also need to write a short, 2 page report about your design decisions as a team, to be uploaded to Moodle in the form of a PDF file next to this Jupyter notebook.\n",
    "\n",
    "You should provide arguments and justifications for all of your design decisions throughout this investigation. You can use your M3 responses as the basis for this discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea2d32-f0a9-4dc9-bb60-be43399f5b89",
   "metadata": {
    "id": "82ea2d32-f0a9-4dc9-bb60-be43399f5b89"
   },
   "outputs": [],
   "source": [
    "# Import the tables of the data set as dataframes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "\n",
    "DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89137355",
   "metadata": {},
   "source": [
    "## Task 0: Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dafc5b",
   "metadata": {},
   "source": [
    "**Research question:**\n",
    "*Your chosen research question goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f62b0-1945-48f1-8f22-5f6ebda1db8e",
   "metadata": {
    "id": "a77f62b0-1945-48f1-8f22-5f6ebda1db8e"
   },
   "source": [
    "## Task 1: Data Preprocessing\n",
    "\n",
    "In this section, you are asked to preprocess your data in a way that is relevant for the model. Please include 1-2 visualizations of features / data explorations that are related to your downstream prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff5fad",
   "metadata": {},
   "source": [
    "### A) Student Activity (Inputs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73e305",
   "metadata": {},
   "source": [
    "##### 1) Student Activity Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff73320",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "\n",
    "We will begin by loading the `activity` dataset and processing it as follows:\n",
    "\n",
    "- Cast columns to appropriate data types (e.g., convert to datetime formats)  \n",
    "- Remove entries with missing `activity_completed` values or inconsistent timestamps (e.g., when completion occurs before the activity starts)  \n",
    "- Remove entries where the activity duration exceeds 24 hours, as these likely indicate inactivity (e.g., students leaving the activity open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cefd0d-4f8a-4227-8fb4-f30521abf78d",
   "metadata": {
    "id": "34cefd0d-4f8a-4227-8fb4-f30521abf78d"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "activity = pd.read_csv(f'{DATA_DIR}/activity.csv')\n",
    "activity_sizes = {'Original': len(activity)}\n",
    "\n",
    "# Cast timestamps to datetime\n",
    "activity['activity_started'] = pd.to_datetime(activity['activity_started'], unit='s')\n",
    "activity['activity_completed'] = pd.to_datetime(activity['activity_completed'], unit='s')\n",
    "activity['activity_updated'] = pd.to_datetime(activity['activity_updated'], unit='s')\n",
    "\n",
    "# Remove entries where the activity_completed was before the activity_started\n",
    "activity = activity[activity['activity_completed'] > activity['activity_started']]\n",
    "activity_sizes['After removing missing or inconcistent completion'] = len(activity)\n",
    "\n",
    "# Create a column with the duration of the activity\n",
    "activity['activity_duration'] = activity['activity_completed'] - activity['activity_started']\n",
    "activity['activity_duration_minutes'] = activity['activity_duration'].dt.total_seconds() / 60\n",
    "\n",
    "# Remove entries where the activity_completed is more than 24 hours after the activity_started\n",
    "activity = activity[activity['activity_duration_minutes'] <= 24 * 60]\n",
    "activity_sizes['After removing duration > 24h'] = len(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4097d58",
   "metadata": {},
   "source": [
    "In addition to removing entries where the activity duration exceeds 24 hours, we will also eliminate remaining outliers using the IQR method, as it is still unlikely that a student would spend several hours on a single activity. The IQR method provides a statistically grounded threshold for identifying such outliers, removing the need to define a manual cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IQR bounds for filtering\n",
    "Q1 = activity['activity_duration_minutes'].quantile(0.25)\n",
    "Q3 = activity['activity_duration_minutes'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"Activity Duration Distributions\", fontsize=16)\n",
    "\n",
    "# Before filtering\n",
    "sns.histplot(activity['activity_duration_minutes'], bins=30, color='orange', ax=axes[0])\n",
    "axes[0].set_title('Original Distribution')\n",
    "axes[0].set_xlabel('Activity Duration (minutes)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid()\n",
    "\n",
    "# Filter outliers\n",
    "activity = activity[\n",
    "    (activity['activity_duration_minutes'] >= lower_bound) &\n",
    "    (activity['activity_duration_minutes'] <= upper_bound)\n",
    "]\n",
    "activity_sizes['After removing remaining outliers'] = len(activity)\n",
    "\n",
    "# After filtering\n",
    "sns.histplot(activity['activity_duration_minutes'], bins=30, color='orange', ax=axes[1])\n",
    "axes[1].set_title('After Removing Outliers')\n",
    "axes[1].set_xlabel('Activity Duration (minutes)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784265d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset evolution\n",
    "print(\"Data set sizes at each stage:\")\n",
    "for stage, size in activity_sizes.items():\n",
    "    print(f\"- {stage}: {size} entries ({size / activity_sizes['Original'] * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bb757",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "We observe that before applying the IQR method to filter out remaining outliers, the distribution of activity duration is right-skewed, with numerous extreme values reaching up to 1,400 mins (≈ 23 hours).\n",
    "\n",
    "After removing these outliers, the distribution remains right-skewed—likely resembling an exponential distribution, with durations ranging from 0 minutes to 1 hour.\n",
    "\n",
    "Empirically, we found that the IQR method performs more effectively when applied after first removing entries with durations exceeding 24 hours, rather than relying on it alone.\n",
    "\n",
    "After applying all filters, we are left with 22,754 entries—about 31% of the original dataset. The most significant reduction occurs when removing entries with missing or inconsistent completion times, which alone reduces the dataset from 100% to 53%.  \n",
    "\n",
    "One possible solution to retain more data would be to predict the missing completion times. However, this could compromise the reliability of the analysis by introducing less representative user behavior. Therefore, we chose to leave this as a potential direction for future work, if deemed necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d22f9",
   "metadata": {},
   "source": [
    "##### 2) Student Feature Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a273ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_daily_activity = activity.copy(deep=True)\n",
    "\n",
    "# Create new column for activity started day\n",
    "user_daily_activity['active_day'] = pd.to_datetime(user_daily_activity['activity_started'].dt.date)\n",
    "\n",
    "# Drop duplicates in user_id, active_day\n",
    "user_daily_activity.drop_duplicates(subset=['user_id', 'active_day'], inplace=True, keep='first')\n",
    "\n",
    "# Compute gaps between active days for each user\n",
    "user_daily_activity.sort_values(['user_id', 'active_day'], inplace=True)\n",
    "user_daily_activity['previous_day'] = user_daily_activity.groupby('user_id')['active_day'].shift(1)\n",
    "user_daily_activity['gap_days'] = (user_daily_activity['active_day'] - user_daily_activity['previous_day']).dt.days\n",
    "\n",
    "# Keep only relevnt columns\n",
    "columns = ['user_id', 'active_day', 'gap_days']\n",
    "user_daily_activity = user_daily_activity[columns]\n",
    "\n",
    "display(user_daily_activity.head())\n",
    "print(f'Number of unique user entries: {user_daily_activity.user_id.nunique()}')\n",
    "print(f'Number of unique (user, day) entries: {user_daily_activity.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram of number of active days per user\n",
    "sns.histplot(user_daily_activity['user_id'].value_counts(), bins=20, ax=axes[0])\n",
    "axes[0].set_title('Distribution of Number of Active Days per User')\n",
    "axes[0].set_xlabel('Number of Active Days')\n",
    "axes[0].set_ylabel('Number of Users')\n",
    "\n",
    "# Histogram of gaps between active days\n",
    "sns.histplot(user_daily_activity['gap_days'], ax=axes[1])\n",
    "axes[1].set_title('Distribution of Number of Inactive Days')\n",
    "axes[1].set_xlabel('Number of Days')\n",
    "axes[1].set_ylabel('Number of Users')\n",
    "axes[1].set_xlim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521c3e5",
   "metadata": {},
   "source": [
    "user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b160cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the median of gap_days per user\n",
    "user_profile_activity = user_daily_activity.groupby('user_id')['gap_days'].median().reset_index()\n",
    "user_profile_activity.rename(columns={'gap_days': 'median_gap_days'}, inplace=True)\n",
    "\n",
    "# Add the number of active days of that user (count)\n",
    "user_profile_activity['active_days_count'] = user_daily_activity.groupby('user_id')['active_day'].count().values\n",
    "\n",
    "# Add the median activity duration of that user (activity_completed - activity_started) only when activity_completed is not missing\n",
    "user_profile_activity['median_activity_duration'] = activity.groupby('user_id').apply(\n",
    "    lambda x: (x['activity_completed'] - x['activity_started']).median() if not x['activity_completed'].isnull().all() else np.nan\n",
    ").values\n",
    "# Add the number of activities of that user (count)\n",
    "user_profile_activity['activities_count'] = activity.groupby('user_id')['activity_id'].count().values\n",
    "# Add the number of unique activity types of that user\n",
    "user_profile_activity['activity_types_count'] = activity.groupby('user_id')['activity_type'].nunique().values\n",
    "\n",
    "# Convert timedelta to minutes\n",
    "user_profile_activity['median_activity_duration_minutes'] = user_profile_activity['median_activity_duration'].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each column in a suitable seaborn chart using a single figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(\"User Activity Statistics\", fontsize=16)\n",
    "\n",
    "# Histogram for median_gap_days\n",
    "sns.histplot(user_profile_activity['median_gap_days'], ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Median Gap Days')\n",
    "\n",
    "# Histogram for active_days_count\n",
    "sns.histplot(user_profile_activity['active_days_count'], bins=50, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Active Days Count')\n",
    "\n",
    "# Boxplot for median_activity_duration\n",
    "sns.histplot(user_profile_activity['median_activity_duration_minutes'], ax=axes[0, 2], bins=30)\n",
    "axes[0, 2].set_title('Median Activity Duration (minutes)')\n",
    "\n",
    "# Histogram for activities_count\n",
    "sns.histplot(user_profile_activity['activities_count'], ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Activities Count')\n",
    "\n",
    "# Countplot for activity_types_count\n",
    "sns.countplot(x=user_profile_activity['activity_types_count'], ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Activity Types Count')\n",
    "\n",
    "# Hide the last unused subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae434a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Activity per user per day:\")\n",
    "display(user_daily_activity.head())\n",
    "print(\"Shape:\", user_daily_activity.shape)\n",
    "\n",
    "print(\"\\n\\n- User profile:\")\n",
    "display(user_profile_activity.head())\n",
    "print(\"Shape:\", user_profile_activity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9d78d",
   "metadata": {},
   "source": [
    "### B) Student Scores (Outputs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878144f",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "\n",
    "We will begin by loading the `all_scores` dataset and processing it as follows:\n",
    "\n",
    "- Cast columns to appropriate data types (e.g., convert to datetime formats)\n",
    "- Aggregate the scores for each student by computing the median of their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "all_scores = pd.read_csv(f'{DATA_DIR}/all_scores.csv')\n",
    "\n",
    "# Cast timestamps to datetime\n",
    "all_scores['time'] = pd.to_datetime(all_scores['time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute median and count of scores per user\n",
    "students_median_score = all_scores.groupby('user_id')['percentage'].median().reset_index()\n",
    "students_median_score.rename(columns={'percentage': 'median_score'}, inplace=True)\n",
    "students_median_score['scores_count'] = all_scores.groupby('user_id')['percentage'].count().values\n",
    "\n",
    "# Remove users with less than 1 score\n",
    "students_median_score = students_median_score[students_median_score['scores_count'] > 1]\n",
    "\n",
    "# Plot 1: Original distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "sns.histplot(students_median_score['scores_count'], bins=50, ax=axes[0])\n",
    "axes[0].set_title('Scores Count per User (All Data)')\n",
    "axes[0].set_xlabel('Number of Scores')\n",
    "axes[0].set_ylabel('Number of Users')\n",
    "axes[0].grid()\n",
    "\n",
    "# IQR Outlier Removal (in-place on df)\n",
    "Q1 = students_median_score['scores_count'].quantile(0.25)\n",
    "Q3 = students_median_score['scores_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter in-place\n",
    "length_before = len(students_median_score)\n",
    "students_median_score = students_median_score[(students_median_score['scores_count'] >= lower_bound) & (students_median_score['scores_count'] <= upper_bound)]\n",
    "length_after = len(students_median_score)\n",
    "\n",
    "# Plot 2: After outlier removal\n",
    "sns.histplot(students_median_score['scores_count'], bins=50, ax=axes[1])\n",
    "axes[1].set_title('Scores Count per User (Outliers Removed)')\n",
    "axes[1].set_xlabel('Number of Scores')\n",
    "axes[1].grid()\n",
    "\n",
    "plt.suptitle('Distribution of Scores Count per User', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of users before outlier removal: {length_before}\")\n",
    "print(f\"Number of users after outlier removal: {length_after} ({length_after / length_before * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc448282",
   "metadata": {},
   "source": [
    "Once again, we removed outlier students—those with very few or very many entries—using the IQR method. This helps eliminate bias from users who participated only once, as well as those who were unusually active and not representative of the majority. While filtering out users with too few entries was necessary, removing the highly active users was optional. In both cases, the distributions are right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6febc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier removal to the all_scores dataframe\n",
    "all_scores = all_scores[all_scores['user_id'].isin(students_median_score['user_id'])]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Distribution of all individual scores\n",
    "sns.histplot(all_scores['percentage'], color='lightblue', ax=axes[0])\n",
    "axes[0].set_title('Distribution of All Scores')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right: Distribution of median scores per user\n",
    "sns.histplot(students_median_score['median_score'], color='lightblue', ax=axes[1])\n",
    "axes[1].set_title('Distribution of Median Scores per User')\n",
    "axes[1].set_xlabel('Median Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef979d",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "The chart on the left shows the distribution of all individual scores. We observe a noisy Gaussian-like distribution with two unusual peaks at 0 and 100. One possible explanation is that the scores are expressed as percentages, and exercises graded out of 1 point can result only in 0% or 100%.\n",
    "\n",
    "On the right, we see the distribution of each student's median score, which follows a normal distribution centered around 75%. Notably, the sharp peaks at 0 and 100 disappear after aggregating by student. This is likely due to the use of the median, which is less sensitive to outliers than the mean.\n",
    "\n",
    "From this point onward, we will consider only the median score per user (i.e., the overall user grade), as shown in the chart on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3736031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final dataframe with user activity and scores\n",
    "students = user_profile_activity.merge(\n",
    "    students_median_score,\n",
    "    how='inner',\n",
    "    left_on='user_id',\n",
    "    right_on='user_id'\n",
    ")\n",
    "\n",
    "# Print number of users\n",
    "print(f\"Number of users in user_profile_activity: {user_profile_activity['user_id'].nunique()}\")\n",
    "print(f\"Number of users in students_median_score: {students_median_score['user_id'].nunique()}\")\n",
    "print(f\"Number of users after merging: {students['user_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44205edb",
   "metadata": {},
   "source": [
    "We end up with a final `students` dataset containing 545 students, which closely matches the size of the scores dataset (95% overlap)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85633adb-d317-4ee3-bf06-e9f82f589c41",
   "metadata": {
    "id": "85633adb-d317-4ee3-bf06-e9f82f589c41"
   },
   "source": [
    "## Task 2: Model Building\n",
    "\n",
    "Train a model for your research question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4417d",
   "metadata": {},
   "source": [
    "### A) Student Clustering (Profiles):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d32ec",
   "metadata": {},
   "source": [
    "We will create clusters of students to better understand the different learner profiles. To achieve this, we will use an unsupervised learning approach with K-Means clustering.\n",
    "\n",
    "The key hyperparameters we will determine are:\n",
    "\n",
    "- **Features used for clustering:** Determined through correlation analysis and machine learning coefficient interpretation.\n",
    "- **Number of clusters (K):** Determined using the Elbow method  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af9e30",
   "metadata": {},
   "source": [
    "##### 1) Hyperparameter Tuning of Clustering Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f07fe",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "\n",
    "- **Corralation Analysis:** As observed in Task A, the scores follow a Gaussian distribution. However, the input features are highly right-skewed and appear to follow an exponential distribution. Therefore, we will use Spearman correlation instead of Pearson, as the latter is only appropriate for normally distributed data.\n",
    "\n",
    "- **Machine Learning Coefficient Interpretation:** We will use a linear regression model to interpret the coefficients and understand which features are most strongly associated with the output. Since the goal is interpretation rather than prediction, we will train the model on the full dataset without splitting it into training and evaluation sets. However, to ensure robustness, we will perform cross-validation and average the coefficient weights across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the candidate features for clustering\n",
    "candidate_features = [\n",
    "    'median_gap_days',\n",
    "    'active_days_count',\n",
    "    'activities_count',\n",
    "    'activity_types_count',\n",
    "    'median_activity_duration_minutes'\n",
    "]\n",
    "\n",
    "# Define the target variable\n",
    "target = 'median_score'\n",
    "\n",
    "# Drop entries with missing values in the candidate features and target (necessary for models)\n",
    "students_no_nan = students.dropna(subset=candidate_features + [target], inplace=False)\n",
    "\n",
    "# Print number of entries after dropping NaN values, and percentage of entries kept\n",
    "print(f\"Number of entries after dropping NaN values: {students_no_nan.shape[0]}\")\n",
    "print(f\"Percentage of entries kept: {students_no_nan.shape[0] / students.shape[0] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Spearman correlation\n",
    "correlations = []\n",
    "for feature in candidate_features:\n",
    "    corr = students_no_nan[[feature, target]].corr(method='spearman').iloc[0, 1]\n",
    "    correlations.append(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54115a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Linear regression coefficients\n",
    "X = students_no_nan[candidate_features]\n",
    "y = students_no_nan[target]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_coefs = []\n",
    "\n",
    "# Cross-validated linear regression\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, y_train = X_scaled[train_index], y.iloc[train_index]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    all_coefs.append(model.coef_)\n",
    "\n",
    "# Average coefficients across folds\n",
    "weights = np.mean(all_coefs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into DataFrame\n",
    "combined_df = pd.DataFrame({\n",
    "    'Feature': candidate_features,\n",
    "    'Pearson Correlation': correlations,\n",
    "    'Linear Regression Weight': weights\n",
    "})\n",
    "\n",
    "# Custom red-to-green gradient\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_red_green\", [\"red\", \"yellow\", \"green\"])\n",
    "norm_corr = Normalize(vmin=min(correlations), vmax=max(correlations))\n",
    "norm_weight = Normalize(vmin=min(weights), vmax=max(weights))\n",
    "\n",
    "# Handle colors\n",
    "corr_colors = [custom_cmap(norm_corr(val)) for val in correlations]\n",
    "weight_colors = [custom_cmap(norm_weight(val)) for val in weights]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# Correlation barplot\n",
    "bars_corr = sns.barplot(x='Pearson Correlation', y='Feature', data=combined_df, ax=axes[0], color='gray')\n",
    "for bar, color in zip(bars_corr.patches, corr_colors):\n",
    "    bar.set_color(color)\n",
    "axes[0].axvline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[0].set_title('Pearson Correlation with median_score')\n",
    "axes[0].set_xlabel('Correlation')\n",
    "axes[0].set_ylabel('Feature')\n",
    "\n",
    "# Linear regression barplot\n",
    "bars_weight = sns.barplot(x='Linear Regression Weight', y='Feature', data=combined_df, ax=axes[1], color='gray')\n",
    "for bar, color in zip(bars_weight.patches, weight_colors):\n",
    "    bar.set_color(color)\n",
    "axes[1].axvline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[1].set_title('Linear Regression Coefficients')\n",
    "axes[1].set_xlabel('Weight')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0f607",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549a5fe",
   "metadata": {},
   "source": [
    "##### 2) Hyperparameter Tuning of Number of Clusters (K):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be22ced0",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "\n",
    "We will use K-Means clustering and determine the optimal number of clusters using the Elbow method. The Euclidean distance metric will be used, as it is well-suited for our data, where all features are numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all features for clustering\n",
    "columns_used = candidate_features\n",
    "X = students_no_nan[columns_used]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Use Elbow method to find optimal K\n",
    "inertia = []\n",
    "K_range = range(1, 11)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Select optimal K\n",
    "knee = KneeLocator(K_range, inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k = knee.knee\n",
    "\n",
    "# Apply KMeans with optimal K\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "students_no_nan = students_no_nan.copy()\n",
    "students_no_nan['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), gridspec_kw={'width_ratios': [1, 2]})\n",
    "\n",
    "# Plot Elbow chart\n",
    "axes[0].plot(K_range, inertia, marker='o')\n",
    "axes[0].axvline(optimal_k, color='red', linestyle='--', label=f'Optimal K = {optimal_k}')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].set_xlabel('K')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Apply PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Add PCA components to the DataFrame\n",
    "students_projection_pca = students_no_nan.copy()\n",
    "students_projection_pca['PCA1'] = X_pca[:, 0]\n",
    "students_projection_pca['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "# Plot Cluster scatterplot using PCA components\n",
    "sns.scatterplot(\n",
    "    data=students_projection_pca,\n",
    "    x='PCA1',\n",
    "    y='PCA2',\n",
    "    hue='cluster',\n",
    "    palette='tab10',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(f\"KMeans Clustering (PCA Projection, K={optimal_k})\")\n",
    "axes[1].set_xlabel(\"Principal Component 1\")\n",
    "axes[1].set_ylabel(\"Principal Component 2\")\n",
    "axes[1].legend(title='Cluster')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228b8b5",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "As shown in the chart on the left, after selecting the clustering features, we used the Elbow method with K-Means to determine that the optimal number of clusters is **4**.\n",
    "\n",
    "The chart on the right presents a PCA projection of the data onto two dimensions, with the resulting clusters visualized. Note that the clustering was performed on the original feature space, not on the 2D projection, which is used solely for visualization purposes.\n",
    "\n",
    "TODO: Interpret the right chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e9296",
   "metadata": {},
   "source": [
    "##### 3) Score Comparison Across Clusters and Interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7333a",
   "metadata": {},
   "source": [
    "Now that we have our clusters, we will compare student scores across them by analyzing the median grade within each cluster. Specifically, we will compute the median of each feature across all students in each cluster. This aggregation approach helps reduce the impact of outliers and provides a more robust comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23157181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median feature values per cluster\n",
    "cluster_values = students_no_nan.groupby('cluster')[columns_used].median().T\n",
    "cluster_values.columns = [f'Cluster {c}' for c in cluster_values.columns]\n",
    "\n",
    "# Add median score and user count\n",
    "cluster_score = students_no_nan.groupby('cluster')['median_score'].median().reset_index()\n",
    "cluster_score['users_count'] = students_no_nan.groupby('cluster')['user_id'].nunique().values\n",
    "cluster_score = cluster_score.set_index('cluster').T\n",
    "cluster_score.columns = [f'Cluster {c}' for c in cluster_score.columns]\n",
    "\n",
    "# Combine values + scores\n",
    "cluster_values_df = pd.concat([cluster_values, cluster_score], axis=0)\n",
    "\n",
    "# Display numeric summary\n",
    "display(cluster_values_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c36f98",
   "metadata": {},
   "source": [
    "**Interpretation and Visualization Approach:**\n",
    "\n",
    "The features will be interpreted using simple, color-coded labels for easier readability. Users will be divided into three equally sized groups (low, medium, high) based on how their feature values compare to others. For features with a clear positive or negative impact, text colors will reflect the interpretation (e.g., green for favorable values, red for unfavorable ones), while neutral features will use plain labels.  \n",
    "\n",
    "Additionally, clusters (profiles) will be ordered in decreasing order of their median score to facilitate interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_interpretation_df(use_auto_colors, interpretation_labels, feature_to_corr=None, custom_cmap=None):\n",
    "    # Rename cluster columns to \"Profile {n+1}\"\n",
    "    profile_values = cluster_values.copy()\n",
    "    profile_values.columns = [f'Profile {int(c.split()[-1]) + 1}' for c in profile_values.columns]\n",
    "    \n",
    "    interpretation_df = pd.DataFrame(index=profile_values.index, columns=profile_values.columns, dtype=object)\n",
    "\n",
    "    for feature in interpretation_df.index:\n",
    "        data = students_no_nan[feature]\n",
    "        values = profile_values.loc[feature]\n",
    "\n",
    "        if feature == 'median_gap_days':\n",
    "            data = -data\n",
    "            values = -values\n",
    "\n",
    "        _, bins = pd.qcut(data, q=3, retbins=True, duplicates='drop')\n",
    "        labels = interpretation_labels.get(feature, ['Low', 'Medium', 'High'])[:len(bins) - 1]\n",
    "\n",
    "        for profile in profile_values.columns:\n",
    "            bin_index = pd.cut([values[profile]], bins=bins, labels=range(len(labels)), include_lowest=True)[0]\n",
    "            label_text = labels[int(bin_index)]\n",
    "\n",
    "            if use_auto_colors:\n",
    "                corr = feature_to_corr.get(feature, 0)\n",
    "                if corr >= 0:\n",
    "                    color = custom_cmap(int(bin_index) / 2)\n",
    "                else:\n",
    "                    reversed_bin = 2 - int(bin_index)\n",
    "                    color = custom_cmap(reversed_bin / 2)\n",
    "            else:\n",
    "                color_map = {'Red': 'red', 'Yellow': 'orange', 'Green': 'green'}\n",
    "                label_text, label_color = interpretation_labels[feature][int(bin_index)]\n",
    "                color = color_map[label_color]\n",
    "\n",
    "            interpretation_df.loc[feature, profile] = (label_text, color)\n",
    "\n",
    "    interpretation_df.loc['median_score'] = [(f\"{val:.1f}\", None) for val in median_score_row]\n",
    "    interpretation_df.loc['users_count'] = [(f\"{int(val)}\", None) for val in users_count_row]\n",
    "    interpretation_df.index = interpretation_df.index.map(lambda f: feature_display_names.get(f, f))\n",
    "\n",
    "    return interpretation_df\n",
    "\n",
    "def plot_interpretation_table(ax, interpretation_df, title):\n",
    "    sorted_profiles = median_score_row[interpretation_df.columns].sort_values(ascending=False).index.tolist()\n",
    "    ordered_df = interpretation_df[sorted_profiles]\n",
    "    student_counts = interpretation_df.loc[\"Number of Students\"]\n",
    "    student_counts = student_counts[sorted_profiles]\n",
    "    col_headers = [f\"Profile {i+1}\\n({int(float(student_counts[col][0]))} students)\" for i, col in enumerate(sorted_profiles)]\n",
    "\n",
    "    display_df = ordered_df.drop(index=\"Number of Students\")\n",
    "    color_df = ordered_df.drop(index=\"Number of Students\")\n",
    "    display_df = display_df.map(lambda x: x[0] if isinstance(x, tuple) else x)\n",
    "\n",
    "    ax.axis('off')\n",
    "    table = ax.table(\n",
    "        cellText=display_df.values,\n",
    "        rowLabels=display_df.index,\n",
    "        colLabels=col_headers,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 1.8)\n",
    "\n",
    "    for (row, col), cell in table.get_celld().items():\n",
    "        if row == 0:\n",
    "            cell.set_height(0.18)\n",
    "            continue\n",
    "        row_label = display_df.index[row - 1]\n",
    "        if row_label == \"Median Score\":\n",
    "            cell.get_text().set_fontweight('bold')\n",
    "            cell.get_text().set_color('black')\n",
    "            continue\n",
    "        value = color_df.iloc[row - 1, col]\n",
    "        if isinstance(value, tuple) and value[1] is not None:\n",
    "            cell.get_text().set_color(value[1])\n",
    "\n",
    "    median_index = display_df.index.get_loc(\"Median Score\") + 1\n",
    "    row_label_cell = table[(median_index, -1)]\n",
    "    row_label_cell.get_text().set_fontweight('bold')\n",
    "    row_label_cell.get_text().set_color('black')\n",
    "\n",
    "    for row in range(1, len(display_df.index) + 1):\n",
    "        label_cell = table[(row, -1)]\n",
    "        label_cell.get_text().set_color('black')\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Setup\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_red_orange_green\", [\"red\", \"orange\", \"green\"])\n",
    "feature_to_corr = dict(zip(candidate_features, correlations))\n",
    "\n",
    "# Manual interpretation labels\n",
    "manual_labels = {\n",
    "    'median_gap_days': [('Rarely', 'Red'), ('Occasionally', 'Yellow'), ('Frequently', 'Green')],\n",
    "    'active_days_count': [('Low Activity', 'Red'), ('Moderate Activity', 'Yellow'), ('High Activity', 'Green')],\n",
    "    'activities_count': [('Few Activities', 'Red'), ('Some Activities', 'Yellow'), ('Many Activities', 'Green')],\n",
    "    'median_activity_duration_minutes': [('Short Duration', 'Red'), ('Moderate Duration', 'Yellow'), ('Long Duration', 'Green')],\n",
    "    'activity_types_count': [('Narrow Focus', 'Red'), ('Moderate Variety', 'Yellow'), ('Diverse Activity', 'Green')],\n",
    "}\n",
    "\n",
    "auto_labels = {\n",
    "    'median_gap_days': ['Rarely', 'Occasionally', 'Frequently'],\n",
    "    'active_days_count': ['Low Activity', 'Moderate Activity', 'High Activity'],\n",
    "    'activities_count': ['Few Activities', 'Some Activities', 'Many Activities'],\n",
    "    'median_activity_duration_minutes': ['Short Duration', 'Moderate Duration', 'Long Duration'],\n",
    "    'activity_types_count': ['Narrow Focus', 'Moderate Variety', 'Diverse Activity'],\n",
    "}\n",
    "\n",
    "# Prepare shared cluster_score rows\n",
    "adjusted_cluster_score = cluster_score.copy()\n",
    "adjusted_cluster_score.columns = [f'Profile {int(c.split()[-1]) + 1}' for c in adjusted_cluster_score.columns]\n",
    "median_score_row = adjusted_cluster_score.loc['median_score']\n",
    "users_count_row = adjusted_cluster_score.loc['users_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both interpretation DataFrames\n",
    "manual_df = create_interpretation_df(False, manual_labels)\n",
    "auto_df = create_interpretation_df(True, auto_labels, feature_to_corr, custom_cmap)\n",
    "\n",
    "# Plot first table in its own figure\n",
    "fig1, ax1 = plt.subplots(figsize=(12, 4))\n",
    "plot_interpretation_table(ax1, manual_df, \"Interpretation (Manual Color - Common Sense)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot second table in a separate figure\n",
    "fig2, ax2 = plt.subplots(figsize=(12, 4))\n",
    "plot_interpretation_table(ax2, auto_df, \"Interpretation (Automatic Color - Based on Coefficients)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead7f00",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "TODO: Interprete the clusters and explain why they are not perfect. Use also the feature coefficient done before to explain ? Demander au TA d'abord s'il faut garder les couleurs logiques (celles qui donnent actuellement ces résultats), ou bien celle des coefficient (qui seront plus cohérente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9a655-cec9-4c57-aec7-7a982f57a3af",
   "metadata": {
    "id": "b3c9a655-cec9-4c57-aec7-7a982f57a3af"
   },
   "source": [
    "## Task 3: Model Evaluation\n",
    "In this task, you will use metrics to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65720b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for model evaluation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62968daa",
   "metadata": {},
   "source": [
    "*Your discussion/interpretation about your model's behavior goes here*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e49d1dad",
   "metadata": {},
   "source": [
    "## Task 4: Team Reflection\n",
    "Please describe the contributions of each team member to Milestone 4. Reflect on how you worked as team: what went well, what can be improved for the next milestone?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cde86a72",
   "metadata": {},
   "source": [
    "*Your discussion about team responsibilities goes here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "m2-classtime-sciper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlbehavior_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
